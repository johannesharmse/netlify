---
title: All Humans are Equal... but Some are More Equal than Others
subtitle: The truth about the democracy of internet privacy
---

# All Humans are Equal... but Some are More Equal than Others

We as humans generally consider ourselves as the most advanced race in the universe. Whilst currently entering [the fourth industrial revolution](https://www.weforum.org/agenda/2016/01/the-fourth-industrial-revolution-what-it-means-and-how-to-respond/), it is difficult to imagine that discrimination still has a big presence in the world.

It is easy to punish individuals who do not respect the human rights of others, but how do we combat discrimination that is driven by [algorithms](https://www.theguardian.com/technology/2016/dec/19/discrimination-by-algorithm-scientists-devise-test-to-detect-ai-bias)?

As part of the community that builds and implements machine learning models, I have come to realize that we have been gifted with a great amount of power. Machine learning models are what drives [automated decision making](https://becominghuman.ai/how-artificial-intelligence-will-change-decision-making-for-businesses-96d47cde98df). We can either build these models to benefit all of its users or to benefit certain subgroups of users, while some users get discriminated against by the models that we build.

Machine learning models can only discriminate based on the data that it is being trained on. If a model is trained to differentiate between users of different ethnicities, we should make sure that model isn't being implemented in a discriminating way. As the people responsible for training these models, we are, in a way, embedding our views into these systems as explained by [Joy Buolamwini](https://medium.com/mit-media-lab/incoding-in-the-beginning-4e2a5c51a45d).

From a citizen's perspective, one might ask why we need to distinguish between people on things like race and gender in our machine learning models. This question can be answered with another question - what is the objective of the model? If we are talking about Facebook's facial recognition software that has the objective of correctly identifying people in a photo, looking at a person's skin colour and facial features should be a completely reasonable thing to do.

However, as data scientists we should ask the question of where do we draw the line? If we take the hypothetical scenario of police using facial recognition software to identify potential criminals in public spaces, would it be considered discriminating if bias is given towards people of darker skin colour? Crime statistics may indicate that criminals are more likely to be black in certain countries, but that does not mean that all criminals are black.

![http://austelagencies.com/files/2018/01/facial-recognition-crowd-image-www.intelagencies.com_.jpg](img/crowd.jpg)

Another example to consider is the process of granting loans. If a data scientist can prove that a model can predict with an accuracy score of 98% whether an individual will be able to pay back the loan, should we implement this model? What if the highest weighted feature for this model is the neighbourhood that the individual is from? Neighbourhood may be strongly correlated with financial status, but if it isn't perfectly correlated, this model can be considered discriminating. People can be disadvantaged greatly based on an algorithm that groups people by neighbourhood.

As we are entering the era of machine learning and artificial intelligence implementation, we should start having wider discussions around the algorithms that lie behind automated decision making.

Just because an algorithm isn't a conscious being, we shouldn't assume that it can't be discriminating. It is up to it's creators to make sure that it does not contain a discriminating gene.
